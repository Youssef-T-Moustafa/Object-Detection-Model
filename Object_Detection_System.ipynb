{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Qz5W8O2kFz9x9XkPB0a4DPBGGXjNMvvU",
      "authorship_tag": "ABX9TyPMhIzYaGGoE/hPkEfRI/Cs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youssef-T-Moustafa/Object-Detection-Model/blob/main/Object_Detection_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------\n",
        "# **OBJECT DETECTION PROJECT**\n",
        "## **Made by:** *Youssef Moustafa*\n",
        "-------\n",
        "\n"
      ],
      "metadata": {
        "id": "sXee6nlRhCPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "| Number | Content                                               |\n",
        "|--------|-------------------------------------------------------|\n",
        "| 1      | [Project Overview](#project-overview)                |\n",
        "| 2      | [Project Setup](#project-setup)                      |\n",
        "| 3      | [Import Dependencies](#import-dependencies)          |\n",
        "| 4      | [Clone Darknet Repo and Install Dependencies](#clone-darknet-repo-and-install-dependencies) |\n",
        "| 5      | [Enable GPU, OPENCV, and LIBSO in Darknet Makefile](#enable-gpu-opencv-and-libso-in-darknet-makefile) |\n",
        "| 6      | [Download and Upload Darknet Files](#download-and-upload-darknet-files) |\n",
        "| 7      | [YOLOv4 Object Detection on Sample Image](#yolov4-object-detection-on-sample-image) |\n",
        "| 8      | [Webcam Photo Capture with YOLOv4 Object Detection](#webcam-photo-capture-with-yolov4-object-detection) |\n",
        "| 9      | [Image Upload for YOLOv4 Object Detection](#image-upload-for-yolov4-object-detection) |\n",
        "| 10     | [Live Webcam Object Detection with YOLOv4](#live-webcam-object-detection-with-yolov4) |\n",
        "| 11     | [Conclusion](#conclusion)                            |\n",
        "| 12     | [Future Improvements](#future-improvements)          |\n"
      ],
      "metadata": {
        "id": "EV3mBO1fl7Wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "**Link to Drive:** https://drive.google.com/drive/folders/13VMil-1IZN_CcH5tpZRp4wkoffPuSTeW?usp=sharing\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "S0YTGhiygG1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Object Detection System**\n",
        "\n",
        "The Object Detection System is a comprehensive project designed for real-time object detection using the state-of-the-art YOLOv4 architecture. This system leverages the capabilities of Google Colab for an interactive and accessible environment.\n",
        "\n",
        "# **Project Overview**\n",
        "\n",
        "The system encompasses the following key components and functionalities:\n",
        "\n",
        "1. **Darknet Integration:**\n",
        "   - Utilizes the Darknet framework, a neural network library, and its Python bindings for seamless integration with the YOLOv4 architecture.\n",
        "\n",
        "2. **Webcam Capture and Image Upload:**\n",
        "   - Provides flexibility for users to capture live images from their webcam or upload images directly from their local machine for object detection.\n",
        "\n",
        "3. **Real-time Object Detection:**\n",
        "   - Implements the YOLOv4 architecture for real-time object detection on both live webcam feeds and uploaded images.\n",
        "\n",
        "4. **Interactive Visualization:**\n",
        "   - Dynamically overlays bounding boxes and labels on the video stream, showcasing detected objects in real-time.\n",
        "\n",
        "5. **User-friendly Experience:**\n",
        "- Employs JavaScript and Python interaction to create an intuitive and interactive interface for capturing images, running object detection, and visualizing results."
      ],
      "metadata": {
        "id": "jjUDOkB-lc7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Setup**\n",
        "To ensure the successful execution of the PRISM Object Detection Project, follow these setup steps:\n",
        "\n",
        "1. **Change Runtime Accelerator to T4 GPU:**\n",
        "   - Navigate to the Colab notebook menu and select \"Runtime.\"\n",
        "   - Choose \"Change runtime type\" and set the hardware accelerator to \"GPU\" (specifically, T4 GPU).\n",
        "\n",
        "2. **Download Necessary Files:**\n",
        "   - Download the following files from the drive link and store them on **your Google Drive**:\n",
        "     - darnket.py\n",
        "     - yolov4-csp.weights\n",
        "  - Copy their paths into the respective cells where they are required.\n",
        "\n",
        "3. **Mount Google Drive:**\n",
        "   - Mount your Google Drive to the Colab notebook using the following code snippet:\n",
        "     ```python\n",
        "     from google.colab import drive\n",
        "     drive.mount('/content/drive')\n",
        "     ```\n",
        "   - Follow the prompted instructions to authenticate and provide access.\n",
        "\n",
        "4. **Enable Webcam**\n",
        "- Webcam is required for the live object detection.\n",
        "\n",
        "Now, your environment is set up, and you're ready to proceed with the PRISM Object Detection Project."
      ],
      "metadata": {
        "id": "xLD0eOCugZf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Dependencies**"
      ],
      "metadata": {
        "id": "iTv_ZzLEinZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYGAmyp4eODP"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import ctypes as ct\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block imports essential libraries and modules for the PRISM Object Detection Project:\n",
        "\n",
        "- **IPython.display:** Functions for displaying outputs, handling JavaScript, and showing images.\n",
        "- **google.colab.output:** Tools for evaluating JavaScript code and displaying OpenCV images.\n",
        "- **base64:** Functions for encoding and decoding base64 data.\n",
        "- **tensorflow (as tf):** TensorFlow, a machine learning library for deep learning tasks.\n",
        "- **cv2:** OpenCV, a computer vision library for image and video processing.\n",
        "- **ctypes (as ct):** C-compatible data types, useful for low-level operations.\n",
        "- **random:** Module for generating random numbers, often used in data augmentation.\n",
        "- **os:** Operating system interaction for file and directory operations.\n",
        "- **numpy (as np):** NumPy, a numerical computing library for array handling.\n",
        "- **PIL:** Python Imaging Library, used for image processing tasks.\n",
        "- **io:** Module for working with streams and file-like objects.\n",
        "- **html:** Module for HTML-related operations.\n",
        "- **time:** Module for time-related operations.\n",
        "- **shutil:** Module for high-level file operations like moving and copying.\n",
        "- **matplotlib.pyplot (as plt):** Matplotlib, a plotting library for visualizing data.\n",
        "- **%matplotlib inline:** Jupyter magic command for rendering Matplotlib plots in the notebook.\n"
      ],
      "metadata": {
        "id": "B9-3V1TSiixr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clone Darknet Repo and Install Dependencies**"
      ],
      "metadata": {
        "id": "w19VrgAci4EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone darknet repo\n",
        "!git clone https://github.com/AlexeyAB/darknet"
      ],
      "metadata": {
        "id": "EzAmykpafiE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install darknet\n",
        "!pip install darknet.py\n",
        "!pip3 install opencv-python\n",
        "!pip3 install scikit-image\n",
        "import darknet"
      ],
      "metadata": {
        "id": "gno7zYslKtAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code block performs the following tasks:\n",
        "\n",
        "1. **Clone Darknet Repository:**\n",
        "   - Clones the Darknet repository from [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet).\n",
        "\n",
        "2. **Install Darknet Python Bindings:**\n",
        "   - Installs the Darknet library using the `darknet` and `darknet.py` Python packages.\n",
        "   - The `!pip install darknet` and `!pip install darknet.py` commands handle the installation.\n",
        "\n",
        "3. **Install OpenCV:**\n",
        "   - Installs the OpenCV library using the `opencv-python` package.\n",
        "\n",
        "4. **Install scikit-image:**\n",
        "   - Installs the scikit-image library using the `scikit-image` package.\n",
        "\n",
        "5. **Import Darknet:**\n",
        "   - Imports the Darknet library, making its functions and classes available for use in the notebook.\n",
        "\n",
        "These steps set up the Darknet environment and its Python bindings along with additional dependencies for image processing and computer vision tasks.\n"
      ],
      "metadata": {
        "id": "81F36siAi1nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enable GPU, OPENCV, and LIBSO in Darknet Makefile**"
      ],
      "metadata": {
        "id": "wmlkYgDZjHLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change makefile to have GPU, OPENCV and LIBSO enabled\n",
        "%cd darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
        "!sed -i 's/LIBSO=0/LIBSO=1/' Makefile"
      ],
      "metadata": {
        "id": "PvC-3_eNfk5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to build darknet (will take a few minutes)\n",
        "!make"
      ],
      "metadata": {
        "id": "yVeTyb0gUrpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code block modifies the Darknet Makefile to enable GPU support, OpenCV integration, and build the shared library (`libdarknet.so`).\n",
        "\n",
        "1. **Change Directory to Darknet:**\n",
        "   - Switches to the Darknet directory using `%cd darknet`.\n",
        "\n",
        "2. **Modify Makefile:**\n",
        "   - Uses `sed` commands to modify the Makefile:\n",
        "      - Sets `OPENCV=1` to enable OpenCV support.\n",
        "      - Sets `GPU=1` to enable GPU acceleration.\n",
        "      - Sets `CUDNN=1` to enable cuDNN library support.\n",
        "      - Sets `CUDNN_HALF=1` to enable reduced precision (half) computation in cuDNN.\n",
        "      - Sets `LIBSO=1` to build the shared library (`libdarknet.so`).\n",
        "\n",
        "3. **Build Darknet:**\n",
        "   - Executes the `make` command to build Darknet with the specified configurations.\n",
        "\n",
        "These changes optimize Darknet for GPU usage, enable OpenCV for image processing, and build the shared library for use in Python.\n"
      ],
      "metadata": {
        "id": "V3Pl78iRjF2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download and Upload Darknet Files**\n",
        "##**(PleaseFollow the Project Setup!)**\n"
      ],
      "metadata": {
        "id": "2HvzivlQjV9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#script_path ='/content/drive/MyDrive/Colab_Notebooks/PRISM_Project/darknet.py'\n",
        "\n",
        "# PLEASE DOWNLOAD THE \"darknet.py\" FILE THEN UPLOAD TO DRIVE AND PLACE THE PATH\n",
        "script_path = 'path/of/darknet.py'\n",
        "\n",
        "# Use the %run magic command to execute the script\n",
        "%run $script_path"
      ],
      "metadata": {
        "id": "OK9-GKlzWcRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drive_weights_path ='/content/drive/MyDrive/Colab_Notebooks/PRISM_Project/yolov4-csp.weights'\n",
        "\n",
        "#PLEASE DOWNLOAD THE \"yolov4-csp.weights\" FILE THEN UPLOAD TO DRIVE AND PLACE THE PATH\n",
        "drive_weights_path = 'path/of/yolov4-csp.weights'\n",
        "\n",
        "# Destination path in the Colab environment\n",
        "colab_weights_path = '/content/darknet/yolov4-csp.weights'\n",
        "\n",
        "# Copy the weights file\n",
        "shutil.copy(drive_weights_path, colab_weights_path)"
      ],
      "metadata": {
        "id": "zKr4UCk8C9I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block facilitates the download and setup of essential Darknet files:\n",
        "\n",
        "1. **Download \"darknet.py\" Script:**\n",
        "   - The script path is specified using `script_path`.\n",
        "   - It is then executed using the `%run` magic command.\n",
        "\n",
        "2. **Upload \"yolov4-csp.weights\" File:**\n",
        "   - The script requests users to download the \"yolov4-csp.weights\" file and upload it to their Google Drive.\n",
        "   - The path of the uploaded file on Google Drive is set as `drive_weights_path`.\n",
        "\n",
        "3. **Set Destination Paths:**\n",
        "   - `colab_weights_path` is defined as the destination path for the weights file within the Colab environment.\n",
        "\n",
        "4. **Copy Weights File:**\n",
        "   - The `shutil.copy` command is used to copy the weights file from the Google Drive path to the Colab environment.\n",
        "\n",
        "These steps ensure that the necessary Darknet files are available and properly configured for the project.\n"
      ],
      "metadata": {
        "id": "fNbB4Zx2jT_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLOv4 Object Detection on Sample Image**"
      ],
      "metadata": {
        "id": "8ur0ArJgj8ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in YOLOv4 architecture network\n",
        "from darknet import *\n",
        "\n",
        "network, class_names, class_colors = load_network(\"cfg/yolov4-csp.cfg\", \"cfg/coco.data\", \"yolov4-csp.weights\")\n",
        "width = network_width(network)\n",
        "height = network_height(network)\n",
        "\n",
        "# darknet helper function to run detection on image\n",
        "def darknet_helper(img, width, height):\n",
        "  darknet_image = make_image(width, height, 3)\n",
        "  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img_resized = cv2.resize(img_rgb, (width, height),\n",
        "                              interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "  # get image ratios to convert bounding boxes to proper size\n",
        "  img_height, img_width, _ = img.shape\n",
        "  width_ratio = img_width/width\n",
        "  height_ratio = img_height/height\n",
        "\n",
        "  # run model on darknet style image to get detections\n",
        "  copy_image_from_bytes(darknet_image, img_resized.tobytes())\n",
        "  detections = detect_image(network, class_names, darknet_image)\n",
        "  free_image(darknet_image)\n",
        "  return detections, width_ratio, height_ratio"
      ],
      "metadata": {
        "id": "XyJB3Pd3gMvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run test on person.jpg image that comes with repository\n",
        "image = cv2.imread(\"data/person.jpg\")\n",
        "detections, width_ratio, height_ratio = darknet_helper(image, width, height)\n",
        "\n",
        "for label, confidence, bbox in detections:\n",
        "  left, top, right, bottom = bbox2points(bbox)\n",
        "  left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "  cv2.rectangle(image, (left, top), (right, bottom), class_colors[label], 2)\n",
        "  cv2.putText(image, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                    (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                    class_colors[label], 2)\n",
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "t_ZAo-qygSfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code block demonstrates the application of YOLOv4 object detection on an example image (\"data/person.jpg\") using the previously loaded network and helper function:\n",
        "\n",
        "1. **Load YOLOv4 Architecture Network:**\n",
        "   - The YOLOv4 architecture network is loaded using the `load_network` function.\n",
        "   - Network configuration is specified in \"cfg/yolov4-csp.cfg,\" class information in \"cfg/coco.data,\" and weights in \"yolov4-csp.weights.\"\n",
        "\n",
        "2. **Get Network Width and Height:**\n",
        "   - `width` and `height` are obtained from the network using the `network_width` and `network_height` functions.\n",
        "\n",
        "3. **Darknet Helper Function:**\n",
        "   - The `darknet_helper` function is defined to facilitate object detection on an image.\n",
        "   - It resizes the image, converts it to the darknet format, runs the YOLOv4 model, and returns detections along with width and height ratios.\n",
        "\n",
        "4. **Run Test on Sample Image:**\n",
        "   - The code reads the \"data/person.jpg\" image and performs object detection using the `darknet_helper` function.\n",
        "   - Detected objects are annotated on the image with bounding boxes and labels.\n",
        "\n",
        "5. **Display Result:**\n",
        "   - The final annotated image is displayed using the `cv2_imshow` function.\n",
        "\n",
        "This example demonstrates the YOLOv4 object detection capabilities on a sample image, showcasing bounding boxes and labels for detected objects.\n"
      ],
      "metadata": {
        "id": "3c7FU3aKj3My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Webcam Photo Capture with YOLOv4 Object Detection**"
      ],
      "metadata": {
        "id": "DFpwfAp6kOA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "            js_reply: JavaScript object containing image from webcam\n",
        "    Returns:\n",
        "            img: OpenCV BGR image\n",
        "    \"\"\"\n",
        "    # decode base64 image\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    # convert bytes to numpy array\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    # decode numpy array into OpenCV BGR image\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "    return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "            bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "    Returns:\n",
        "          bytes: Base64 image byte string\n",
        "    \"\"\"\n",
        "    # convert array into PIL image\n",
        "    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    # format bbox into png for return\n",
        "    bbox_PIL.save(iobuf, format='png')\n",
        "    # format return string\n",
        "    bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    return bbox_bytes"
      ],
      "metadata": {
        "id": "Hgu2Y-pG0bhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data)\n",
        "\n",
        "  # call darknet helper on webcam image\n",
        "  detections, width_ratio, height_ratio = darknet_helper(img, width, height)\n",
        "\n",
        "  # loop through detections and draw them on webcam image\n",
        "  for label, confidence, bbox in detections:\n",
        "    left, top, right, bottom = bbox2points(bbox)\n",
        "    left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "    cv2.rectangle(img, (left, top), (right, bottom), class_colors[label], 2)\n",
        "    cv2.putText(img, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                      (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                      class_colors[label], 2)\n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "metadata": {
        "id": "6TY7dJ18Ac7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "KyJSF9_-0eet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code block defines functions for capturing photos from a webcam and applying YOLOv4 object detection to the captured images:\n",
        "\n",
        "1. **`js_to_image(js_reply)` Function:**\n",
        "   - Converts a JavaScript object containing an image from the webcam into an OpenCV BGR image.\n",
        "   - Decodes base64 image, converts bytes to a NumPy array, and decodes the array into an OpenCV BGR image.\n",
        "\n",
        "2. **`bbox_to_bytes(bbox_array)` Function:**\n",
        "   - Converts an OpenCV rectangle bounding box image into a base64 byte string.\n",
        "   - The function converts the array into a PIL image, saves it as a PNG, and returns the base64 image byte string.\n",
        "\n",
        "3. **`take_photo(filename='photo.jpg', quality=0.8)` Function:**\n",
        "   - Uses JavaScript to capture a photo from the webcam.\n",
        "   - Calls the `js_to_image` function to convert the JavaScript object into an OpenCV BGR image.\n",
        "   - Applies YOLOv4 object detection using the `darknet_helper` function on the webcam image.\n",
        "   - Draws bounding boxes and labels on the image for detected objects.\n",
        "   - Saves the annotated image to a specified filename.\n",
        "   - Displays the saved image.\n",
        "\n",
        "This code allows users to interactively capture photos from their webcam, apply YOLOv4 object detection, and view the results.\n"
      ],
      "metadata": {
        "id": "WRBxrJvEkMCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Upload for YOLOv4 Object Detection**"
      ],
      "metadata": {
        "id": "2jAT4rU_k3Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Function to upload an image from Colab\n",
        "def upload_image():\n",
        "    uploaded = files.upload()\n",
        "    # Return the first uploaded file\n",
        "    return list(uploaded.keys())[0] if uploaded else None\n",
        "\n",
        "# Function to run YOLOv4 object detection on the uploaded image\n",
        "def detect_objects_on_uploaded_image(uploaded_image_path):\n",
        "    if uploaded_image_path:\n",
        "        # Read the uploaded image\n",
        "        uploaded_image = cv2.imread(uploaded_image_path)\n",
        "\n",
        "        # Call the darknet helper on the uploaded image\n",
        "        detections, _, _ = darknet_helper(uploaded_image, width, height)\n",
        "\n",
        "        # Draw bounding boxes and labels on the image\n",
        "        for label, confidence, bbox in detections:\n",
        "            left, top, right, bottom = bbox2points(bbox)\n",
        "            cv2.rectangle(uploaded_image, (left, top), (right, bottom), class_colors[label], 2)\n",
        "            cv2.putText(uploaded_image, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        class_colors[label], 2)\n",
        "\n",
        "        # Display the image with detections\n",
        "        cv2_imshow(uploaded_image)\n",
        "\n",
        "# Upload image from Colab\n",
        "uploaded_image_path = upload_image()\n",
        "\n",
        "# Run YOLOv4 object detection on the uploaded image\n",
        "detect_objects_on_uploaded_image(uploaded_image_path)\n"
      ],
      "metadata": {
        "id": "lTpQpDAAkcJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code block extends the functionality to allow users to upload an image directly from the Colab notebook and apply YOLOv4 object detection:\n",
        "\n",
        "1. **`upload_image()` Function:**\n",
        "   - Utilizes the `files.upload()` method from the Colab `files` module to prompt the user to upload an image.\n",
        "   - Returns the path of the uploaded image or `None` if no image is uploaded.\n",
        "\n",
        "2. **`detect_objects_on_uploaded_image(uploaded_image_path)` Function:**\n",
        "   - Takes the path of the uploaded image as input.\n",
        "   - Reads the uploaded image using OpenCV.\n",
        "   - Applies the `darknet_helper` function for YOLOv4 object detection.\n",
        "   - Draws bounding boxes and labels on the image for detected objects.\n",
        "   - Displays the image with detection results using `cv2_imshow`.\n",
        "\n",
        "3. **Upload Image and Run Detection:**\n",
        "   - Calls `upload_image()` to allow the user to upload an image.\n",
        "   - Calls `detect_objects_on_uploaded_image()` to run YOLOv4 object detection on the uploaded image.\n",
        "\n",
        "This interactive code allows users to either capture a photo from their webcam or upload an image from their local machine, applying YOLOv4 object detection and visualizing the results.\n"
      ],
      "metadata": {
        "id": "0tzncq0yk1oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Live Webcam Object Detection with YOLOv4**"
      ],
      "metadata": {
        "id": "q2NHyc_ulRJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create the live video stream using webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "w9ZeWYoc0fgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # call our darknet helper on video frame\n",
        "    detections, width_ratio, height_ratio = darknet_helper(frame, width, height)\n",
        "\n",
        "    # loop through detections and draw them on transparent overlay image\n",
        "    for label, confidence, bbox in detections:\n",
        "      left, top, right, bottom = bbox2points(bbox)\n",
        "      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)\n",
        "      bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        class_colors[label], 2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # call our darknet helper on video frame\n",
        "    detections, width_ratio, height_ratio = darknet_helper(frame, width, height)\n",
        "\n",
        "    # loop through detections and draw them on transparent overlay image\n",
        "    for label, confidence, bbox in detections:\n",
        "      left, top, right, bottom = bbox2points(bbox)\n",
        "      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)\n",
        "      bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        class_colors[label], 2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "n8rfUd0G0hCb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "965ce9f4-09e1-4e50-96ae-3e1c6ba0ed97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640;\n",
              "      captureCanvas.height = 480;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "\n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "\n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "\n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "\n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "\n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML =\n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640;\n",
              "      captureCanvas.height = 480;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "\n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "\n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "\n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "\n",
              "      return {'create': preShow - preCreate,\n",
              "              'show': preCapture - preShow,\n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code block establishes a live video stream using the webcam as input and performs real-time object detection using YOLOv4. It integrates JavaScript and Python code to create an interactive experience:\n",
        "\n",
        "1. **JavaScript for Video Streaming:**\n",
        "   - The `video_stream()` function initializes a live video stream from the webcam using JavaScript.\n",
        "   - It creates DOM elements for video display and interaction.\n",
        "\n",
        "2. **JavaScript Functions for Video Frame Processing:**\n",
        "   - `stream_frame(label, bbox)` handles processing each video frame.\n",
        "   - It updates the label and bounding box based on the detection results.\n",
        "   - The function communicates between JavaScript and Python, allowing dynamic updates.\n",
        "\n",
        "3. **Main Python Loop for Real-time Detection:**\n",
        "   - A continuous loop captures and processes video frames.\n",
        "   - The `video_frame` function sends the label and bounding box information to the JavaScript side.\n",
        "   - It receives the processed frame as a base64-encoded image.\n",
        "\n",
        "4. **Overlaying Bounding Boxes on Video Frames:**\n",
        "   - Bounding boxes from YOLOv4 detections are drawn on a transparent overlay.\n",
        "   - The overlay is updated for each frame, creating a dynamic visual representation.\n",
        "\n",
        "This interactive code block enables real-time webcam-based object detection using YOLOv4 and showcases the results with bounding boxes and labels.\n"
      ],
      "metadata": {
        "id": "KuPbwNKolPDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "The integration of Darknet and YOLOv4, along with interactive JavaScript and Python components, provides a robust platform for users to explore object detection capabilities in an accessible manner. The system's real-time overlay of bounding boxes and labels on video streams enhances user experience and understanding.\n",
        "\n",
        "# **Future Improvements**\n",
        "\n",
        "To enhance the model and expand its detection capabilities, consider the following improvements:\n",
        "\n",
        "1. **Include More Classes in the Weights File:**\n",
        "   - Expand the YOLOv4 weights file to include additional object classes relevant to specific use cases. This can be achieved by training the model on a diverse dataset containing the desired classes.\n",
        "\n",
        "2. **Custom Model Training:**\n",
        "   - Train a custom YOLOv4 model on a dataset containing a broader range of object classes. Use transfer learning to fine-tune the pre-trained model on specific classes of interest.\n",
        "\n",
        "3. **Data Augmentation:**\n",
        "   - Augment the existing dataset with various transformations (e.g., rotations, flips, and color adjustments) to improve model generalization and robustness.\n",
        "\n",
        "4. **Optimization for Specific Environments:**\n",
        "   - Tailor the model to perform well in specific environments by training it on datasets representative of those environments. This can improve object detection accuracy under specific conditions.\n",
        "\n",
        "5. **Continuous Model Refinement:**\n",
        "   - Periodically update the model with additional annotated data to adapt to evolving object detection requirements and improve overall accuracy.\n",
        "\n",
        "By implementing these improvements, the Object Detection System can evolve into a more versatile and accurate solution for detecting a wider range of objects across various applications.\n"
      ],
      "metadata": {
        "id": "Yp-VsIMUlwAH"
      }
    }
  ]
}